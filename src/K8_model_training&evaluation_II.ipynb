{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing The Necessary Libraries**"
      ],
      "metadata": {
        "id": "J1RkuGCgduhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import timedelta\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest"
      ],
      "metadata": {
        "id": "hfBnIEIDZ-DU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing & Feature Engineering**"
      ],
      "metadata": {
        "id": "2D59s8dHd0y_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "a2M67qJzZnFI"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"After loading, shape:\", df.shape)\n",
        "    print(\"Columns:\", df.columns.tolist())\n",
        "    print(\"Sample data:\\n\", df.head())\n",
        "    return df\n",
        "\n",
        "def preprocess_time(df, time_col='start_time'):\n",
        "    df[time_col] = pd.to_datetime(df[time_col], unit='ns', errors='coerce')\n",
        "    print(f\"After converting {time_col} to datetime, null count:\", df[time_col].isnull().sum())\n",
        "    df = df.sort_values(time_col)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    df['hour_of_day'] = df[time_col].dt.hour\n",
        "    df['day_of_week'] = df[time_col].dt.dayofweek\n",
        "    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "    if 'entity_id' in df.columns:\n",
        "        df['time_since_last'] = df.groupby('entity_id')[time_col].diff().dt.total_seconds().fillna(0)\n",
        "    else:\n",
        "        df['time_since_last'] = df[time_col].diff().dt.total_seconds().fillna(0)\n",
        "    print(\"After time preprocessing, shape:\", df.shape)\n",
        "    return df\n",
        "\n",
        "def map_to_kubernetes(df):\n",
        "    mapping = {\n",
        "        'average_usage': 'container_memory_usage_bytes',\n",
        "        'maximum_usage': 'container_memory_max_usage_bytes',\n",
        "        'assigned_memory': 'container_spec_memory_limit_bytes',\n",
        "        'page_cache_memory': 'container_memory_cache',\n",
        "        'sample_rate': 'scrape_interval'\n",
        "    }\n",
        "    k8s_mapping = {\n",
        "        'pod': 'The smallest deployable unit in Kubernetes',\n",
        "        'node': 'A worker machine in Kubernetes',\n",
        "        'container': 'A lightweight and portable executable image',\n",
        "        'namespace': 'Virtual cluster in Kubernetes',\n",
        "        'deployment': 'Manages replica sets and provides declarative updates to pods'\n",
        "    }\n",
        "    print(\"Kubernetes concept mapping for reference:\")\n",
        "    for k, v in k8s_mapping.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "    return df, mapping\n",
        "\n",
        "def engineer_features(df):\n",
        "    base_features = ['average_usage','maximum_usage','assigned_memory','page_cache_memory','sample_rate']\n",
        "    # Convert to numeric values\n",
        "    for col in base_features:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    derived_features = []\n",
        "    # Process cpu_usage_distribution if available\n",
        "    if 'cpu_usage_distribution' in df.columns:\n",
        "        def parse_cpu_usage(dist_str):\n",
        "            try:\n",
        "                s = dist_str.strip(\"[]\").strip()\n",
        "                parts = re.split(r'\\s+', s)\n",
        "                values = [float(p) for p in parts if p]\n",
        "                if values:\n",
        "                    return np.mean(values), np.std(values), np.max(values), np.percentile(values,95)\n",
        "                else:\n",
        "                    return np.nan, np.nan, np.nan, np.nan\n",
        "            except:\n",
        "                return np.nan, np.nan, np.nan, np.nan\n",
        "        cpu_stats = df['cpu_usage_distribution'].apply(parse_cpu_usage)\n",
        "        df['cpu_usage_mean'] = [x[0] for x in cpu_stats]\n",
        "        df['cpu_usage_std'] = [x[1] for x in cpu_stats]\n",
        "        df['cpu_usage_max'] = [x[2] for x in cpu_stats]\n",
        "        df['cpu_usage_p95'] = [x[3] for x in cpu_stats]\n",
        "        derived_features.extend(['cpu_usage_mean','cpu_usage_std','cpu_usage_max','cpu_usage_p95'])\n",
        "\n",
        "    # Create derived memory metrics\n",
        "    df['memory_utilization'] = df['average_usage'] / df['assigned_memory']\n",
        "    df['memory_pressure'] = df['maximum_usage'] / df['assigned_memory']\n",
        "    df['cache_ratio'] = df['page_cache_memory'] / df['assigned_memory']\n",
        "\n",
        "    # Calculate changes over time if available\n",
        "    if 'time_since_last' in df.columns and df['time_since_last'].max() > 0:\n",
        "        df.loc[df['time_since_last'] == 0, 'time_since_last'] = np.nan\n",
        "        for feature in base_features:\n",
        "            if feature in df.columns:\n",
        "                df[f'{feature}_change'] = df[feature].diff() / df['time_since_last']\n",
        "                derived_features.append(f'{feature}_change')\n",
        "\n",
        "    # Rolling statistics over different windows\n",
        "    windows = [3, 5, 10]\n",
        "    if 'entity_id' in df.columns:\n",
        "        for window in windows:\n",
        "            for feature in ['memory_utilization', 'memory_pressure']:\n",
        "                df[f'{feature}rolling_mean{window}'] = df.groupby('entity_id')[feature].rolling(window=window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "                df[f'{feature}rolling_std{window}'] = df.groupby('entity_id')[feature].rolling(window=window, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "                derived_features.extend([f'{feature}rolling_mean{window}', f'{feature}rolling_std{window}'])\n",
        "    else:\n",
        "        for window in windows:\n",
        "            for feature in ['memory_utilization', 'memory_pressure']:\n",
        "                df[f'{feature}rolling_mean{window}'] = df[feature].rolling(window=window, min_periods=1).mean()\n",
        "                df[f'{feature}rolling_std{window}'] = df[feature].rolling(window=window, min_periods=1).std()\n",
        "                derived_features.extend([f'{feature}rolling_mean{window}', f'{feature}rolling_std{window}'])\n",
        "\n",
        "    all_features = base_features + derived_features + ['memory_utilization', 'memory_pressure', 'cache_ratio', 'hour_of_day', 'day_of_week', 'is_weekend']\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # Ensure 'failed' is in the dataframe\n",
        "    if 'failed' not in df.columns:\n",
        "        raise ValueError(\"Target column 'failed' not found in the dataset.\")\n",
        "    df['failed'] = df['failed'].astype(int)\n",
        "\n",
        "    # Derive additional failure flags\n",
        "    df['resource_exhaustion'] = ((df['maximum_usage'] > df['assigned_memory'] * 0.9) & (df['failed'] == 1)).astype(int)\n",
        "    df['memory_pressure_failure'] = ((df['memory_pressure'] > 0.85) & (df['failed'] == 1)).astype(int)\n",
        "    df['other_failure'] = ((df['failed'] == 1) & (df['resource_exhaustion'] == 0) & (df['memory_pressure_failure'] == 0)).astype(int)\n",
        "\n",
        "    df.dropna(subset=['failed'], inplace=True)\n",
        "    columns_to_fill = list(set(all_features + ['resource_exhaustion','memory_pressure_failure','other_failure']) - {'failed'})\n",
        "    df[columns_to_fill] = df[columns_to_fill].fillna(0)\n",
        "    print(f\"After handling NaNs, shape is now {df.shape}\")\n",
        "    print(\"Remaining columns:\", df.columns.tolist())\n",
        "    return df, all_features\n",
        "\n",
        "def detect_anomalies(df, features, contamination=0.05):\n",
        "    print(\"Running anomaly detection...\")\n",
        "    X = df[features].copy().fillna(0)\n",
        "    isolation_forest = IsolationForest(contamination=contamination, random_state=42)\n",
        "    df['anomaly_score'] = isolation_forest.fit_predict(X)\n",
        "    df['is_anomaly'] = (df['anomaly_score'] == -1).astype(int)\n",
        "    print(f\"Detected {df['is_anomaly'].sum()} anomalies out of {len(df)} records ({df['is_anomaly'].mean() * 100:.2f}%)\")\n",
        "    if 'failed' in df.columns:\n",
        "        correlation = df['is_anomaly'].corr(df['failed'])\n",
        "        print(f\"Correlation between anomalies and failures: {correlation:.4f}\")\n",
        "    return df\n",
        "\n",
        "def normalize_features(df, features):\n",
        "    scaler = MinMaxScaler()\n",
        "    print(\"Before normalization, shape of features:\", df[features].shape)\n",
        "    df_scaled = df.copy()\n",
        "    df_scaled[features] = scaler.fit_transform(df[features])\n",
        "    print(\"After normalization, shape of features:\", df_scaled[features].shape)\n",
        "    return df_scaled, scaler\n",
        "\n",
        "def create_sequences(data, features, seq_length, target_cols):\n",
        "    X_list = []\n",
        "    y_dict = {target: [] for target in target_cols}\n",
        "\n",
        "    def create_sliding_sequences(arr, targets_arr):\n",
        "        windows = np.lib.stride_tricks.sliding_window_view(arr, window_shape=(seq_length, arr.shape[1]))\n",
        "        windows = windows.squeeze(1)\n",
        "        windows = windows[:-1]\n",
        "        targets = targets_arr[seq_length:]\n",
        "        return windows, targets\n",
        "\n",
        "    if 'entity_id' in data.columns:\n",
        "        grouped = data.groupby('entity_id')\n",
        "        for name, group in grouped:\n",
        "            if len(group) <= seq_length:\n",
        "                continue\n",
        "            arr = group[features].values\n",
        "            targets_arr = group[target_cols].values\n",
        "            if len(arr) > seq_length:\n",
        "                seq_X, seq_targets = create_sliding_sequences(arr, targets_arr)\n",
        "                X_list.append(seq_X)\n",
        "                for idx, target in enumerate(target_cols):\n",
        "                    y_dict[target].append(seq_targets[:, idx])\n",
        "    else:\n",
        "        arr = data[features].values\n",
        "        targets_arr = data[target_cols].values\n",
        "        if len(arr) > seq_length:\n",
        "            X_arr, targets = create_sliding_sequences(arr, targets_arr)\n",
        "            X_list.append(X_arr)\n",
        "            for idx, target in enumerate(target_cols):\n",
        "                y_dict[target] = targets[:, idx]\n",
        "    if not X_list:\n",
        "        print(\"No sequences could be created. Possibly not enough data per entity.\")\n",
        "        return np.array([]), {target: np.array([]) for target in target_cols}\n",
        "    X = np.concatenate(X_list, axis=0)\n",
        "    for target in target_cols:\n",
        "        if isinstance(y_dict[target], list):\n",
        "            y_dict[target] = np.concatenate(y_dict[target], axis=0)\n",
        "    print(\"Created sequences: X shape =\", X.shape)\n",
        "    for target, y in y_dict.items():\n",
        "        if len(y) > 0:\n",
        "            print(f\"y_{target} shape =\", y.shape)\n",
        "            print(f\"Positive samples for {target}: {np.sum(y)} ({np.sum(y)/len(y)*100:.2f}%)\")\n",
        "        else:\n",
        "            print(f\"No targets for {target}.\")\n",
        "    return X, y_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def engineer_features(df):\n",
        "    base_features = ['average_usage','maximum_usage','assigned_memory','page_cache_memory','sample_rate']\n",
        "    for col in base_features:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    derived_features = []\n",
        "    if 'cpu_usage_distribution' in df.columns:\n",
        "        def parse_cpu_usage(dist_str):\n",
        "            try:\n",
        "                s = dist_str.strip(\"[]\").strip()\n",
        "                parts = re.split(r'\\s+', s)\n",
        "                values = [float(p) for p in parts if p]\n",
        "                if values:\n",
        "                    return np.mean(values), np.std(values), np.max(values), np.percentile(values,95)\n",
        "                else:\n",
        "                    return np.nan, np.nan, np.nan, np.nan\n",
        "            except:\n",
        "                return np.nan, np.nan, np.nan, np.nan\n",
        "        cpu_stats = df['cpu_usage_distribution'].apply(parse_cpu_usage)\n",
        "        df['cpu_usage_mean'] = [x[0] for x in cpu_stats]\n",
        "        df['cpu_usage_std'] = [x[1] for x in cpu_stats]\n",
        "        df['cpu_usage_max'] = [x[2] for x in cpu_stats]\n",
        "        df['cpu_usage_p95'] = [x[3] for x in cpu_stats]\n",
        "        derived_features.extend(['cpu_usage_mean','cpu_usage_std','cpu_usage_max','cpu_usage_p95'])\n",
        "\n",
        "    df['memory_utilization'] = df['average_usage'] / df['assigned_memory']\n",
        "    df['memory_pressure'] = df['maximum_usage'] / df['assigned_memory']\n",
        "    df['cache_ratio'] = df['page_cache_memory'] / df['assigned_memory']\n",
        "\n",
        "    if 'time_since_last' in df.columns and df['time_since_last'].max() > 0:\n",
        "        df.loc[df['time_since_last'] == 0, 'time_since_last'] = np.nan\n",
        "        for feature in base_features:\n",
        "            if feature in df.columns:\n",
        "                df[f'{feature}_change'] = df[feature].diff() / df['time_since_last']\n",
        "                derived_features.append(f'{feature}_change')\n",
        "\n",
        "    windows = [3, 5, 10]\n",
        "    if 'entity_id' in df.columns:\n",
        "        for window in windows:\n",
        "            for feature in ['memory_utilization', 'memory_pressure']:\n",
        "                df[f'{feature}rolling_mean{window}'] = df.groupby('entity_id')[feature].rolling(window=window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "                df[f'{feature}rolling_std{window}'] = df.groupby('entity_id')[feature].rolling(window=window, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "                derived_features.extend([f'{feature}rolling_mean{window}', f'{feature}rolling_std{window}'])\n",
        "    else:\n",
        "        for window in windows:\n",
        "            for feature in ['memory_utilization', 'memory_pressure']:\n",
        "                df[f'{feature}rolling_mean{window}'] = df[feature].rolling(window=window, min_periods=1).mean()\n",
        "                df[f'{feature}rolling_std{window}'] = df[feature].rolling(window=window, min_periods=1).std()\n",
        "                derived_features.extend([f'{feature}rolling_mean{window}', f'{feature}rolling_std{window}'])\n",
        "\n",
        "    all_features = base_features + derived_features + ['memory_utilization', 'memory_pressure', 'cache_ratio', 'hour_of_day', 'day_of_week', 'is_weekend']\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    if 'failed' not in df.columns:\n",
        "        raise ValueError(\"Target column 'failed' not found in the dataset.\")\n",
        "    df['failed'] = df['failed'].astype(int)\n",
        "\n",
        "    # Updated thresholds:\n",
        "    df['resource_exhaustion'] = ((df['maximum_usage'] >= df['assigned_memory'] * 0.5) & (df['failed'] == 1)).astype(int)\n",
        "    df['memory_pressure_failure'] = ((df['memory_pressure'] >= 0.5) & (df['failed'] == 1)).astype(int)\n",
        "    df['other_failure'] = ((df['failed'] == 1) & (df['resource_exhaustion'] == 0) & (df['memory_pressure_failure'] == 0)).astype(int)\n",
        "\n",
        "    df.dropna(subset=['failed'], inplace=True)\n",
        "    columns_to_fill = list(set(all_features + ['resource_exhaustion','memory_pressure_failure','other_failure']) - {'failed'})\n",
        "    df[columns_to_fill] = df[columns_to_fill].fillna(0)\n",
        "    print(f\"After handling NaNs, shape is now {df.shape}\")\n",
        "    print(\"Remaining columns:\", df.columns.tolist())\n",
        "    return df, all_features\n"
      ],
      "metadata": {
        "id": "xFzWhgck1dfF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the data\n",
        "df = load_data(\"/content/google-cluster-dataset.csv\")\n",
        "\n",
        "# Step 2: Preprocess time columns (using 'start_time' as the time column)\n",
        "df = preprocess_time(df, time_col='start_time')\n",
        "\n",
        "# Step 3: Map to Kubernetes (for reference)\n",
        "df, k8s_mapping = map_to_kubernetes(df)\n",
        "\n",
        "# Step 4: Engineer features and add failure flags\n",
        "df, features = engineer_features(df)\n",
        "\n",
        "# Step 5: Detect anomalies using the engineered features\n",
        "df = detect_anomalies(df, features)\n",
        "\n",
        "# Step 6: Normalize the features for modeling\n",
        "df_scaled, scaler = normalize_features(df, features)\n",
        "\n",
        "# You can now inspect the final DataFrame:\n",
        "df_scaled.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7zTFsWQbaE8L",
        "outputId": "8376b28d-b070-496d-a58c-b1bfd915d3d9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After loading, shape: (405894, 34)\n",
            "Columns: ['Unnamed: 0', 'time', 'instance_events_type', 'collection_id', 'scheduling_class', 'collection_type', 'priority', 'alloc_collection_id', 'instance_index', 'machine_id', 'resource_request', 'constraint', 'collections_events_type', 'user', 'collection_name', 'collection_logical_name', 'start_after_collection_ids', 'vertical_scaling', 'scheduler', 'start_time', 'end_time', 'average_usage', 'maximum_usage', 'random_sample_usage', 'assigned_memory', 'page_cache_memory', 'cycles_per_instruction', 'memory_accesses_per_instruction', 'sample_rate', 'cpu_usage_distribution', 'tail_cpu_usage_distribution', 'cluster', 'event', 'failed']\n",
            "Sample data:\n",
            "    Unnamed: 0           time  instance_events_type  collection_id  \\\n",
            "0           0              0                     2    94591244395   \n",
            "1           1  2517305308183                     2   260697606809   \n",
            "2           2   195684022913                     6   276227177776   \n",
            "3           3              0                     2    10507389885   \n",
            "4           4  1810627494172                     3    25911621841   \n",
            "\n",
            "   scheduling_class  collection_type  priority  alloc_collection_id  \\\n",
            "0                 3                1       200                    0   \n",
            "1                 2                0       360         221495397286   \n",
            "2                 2                0       103                    0   \n",
            "3                 3                0       200                    0   \n",
            "4                 2                0         0                    0   \n",
            "\n",
            "   instance_index    machine_id  ... assigned_memory page_cache_memory  \\\n",
            "0             144  168846390496  ...        0.014435          0.000415   \n",
            "1             335      85515092  ...        0.000000          0.000000   \n",
            "2             376  169321752432  ...        0.010422          0.000235   \n",
            "3            1977  178294817221  ...        0.041626          0.000225   \n",
            "4            3907  231364893292  ...        0.000272          0.000010   \n",
            "\n",
            "   cycles_per_instruction memory_accesses_per_instruction sample_rate  \\\n",
            "0                     NaN                             NaN         1.0   \n",
            "1                     NaN                             NaN         1.0   \n",
            "2                0.939919                        0.001318         1.0   \n",
            "3                1.359102                        0.007643         1.0   \n",
            "4                     NaN                             NaN         1.0   \n",
            "\n",
            "                              cpu_usage_distribution  \\\n",
            "0  [0.00314331 0.00381088 0.00401306 0.00415039 0...   \n",
            "1  [1.23977661e-05 1.23977661e-05 1.23977661e-05 ...   \n",
            "2  [0.01344299 0.01809692 0.0201416  0.02246094 0...   \n",
            "3  [0.03704834 0.04125977 0.04290771 0.04425049 0...   \n",
            "4  [0.         0.         0.         0.         0...   \n",
            "\n",
            "                         tail_cpu_usage_distribution  cluster     event  \\\n",
            "0  [0.00535583 0.00541687 0.00548553 0.00554657 0...        7      FAIL   \n",
            "1  [1.23977661e-05 1.23977661e-05 1.23977661e-05 ...        7      FAIL   \n",
            "2  [0.02902222 0.02929688 0.0295105  0.0296936  0...        7  SCHEDULE   \n",
            "3  [0.05535889 0.05584717 0.05633545 0.05718994 0...        8      FAIL   \n",
            "4  [0.00041485 0.00041485 0.00041485 0.00041485 0...        2    FINISH   \n",
            "\n",
            "   failed  \n",
            "0       1  \n",
            "1       1  \n",
            "2       0  \n",
            "3       1  \n",
            "4       0  \n",
            "\n",
            "[5 rows x 34 columns]\n",
            "After converting start_time to datetime, null count: 0\n",
            "After time preprocessing, shape: (405894, 38)\n",
            "Kubernetes concept mapping for reference:\n",
            "  pod: The smallest deployable unit in Kubernetes\n",
            "  node: A worker machine in Kubernetes\n",
            "  container: A lightweight and portable executable image\n",
            "  namespace: Virtual cluster in Kubernetes\n",
            "  deployment: Manages replica sets and provides declarative updates to pods\n",
            "After handling NaNs, shape is now (405894, 65)\n",
            "Remaining columns: ['Unnamed: 0', 'time', 'instance_events_type', 'collection_id', 'scheduling_class', 'collection_type', 'priority', 'alloc_collection_id', 'instance_index', 'machine_id', 'resource_request', 'constraint', 'collections_events_type', 'user', 'collection_name', 'collection_logical_name', 'start_after_collection_ids', 'vertical_scaling', 'scheduler', 'start_time', 'end_time', 'average_usage', 'maximum_usage', 'random_sample_usage', 'assigned_memory', 'page_cache_memory', 'cycles_per_instruction', 'memory_accesses_per_instruction', 'sample_rate', 'cpu_usage_distribution', 'tail_cpu_usage_distribution', 'cluster', 'event', 'failed', 'hour_of_day', 'day_of_week', 'is_weekend', 'time_since_last', 'cpu_usage_mean', 'cpu_usage_std', 'cpu_usage_max', 'cpu_usage_p95', 'memory_utilization', 'memory_pressure', 'cache_ratio', 'average_usage_change', 'maximum_usage_change', 'assigned_memory_change', 'page_cache_memory_change', 'sample_rate_change', 'memory_utilizationrolling_mean3', 'memory_utilizationrolling_std3', 'memory_pressurerolling_mean3', 'memory_pressurerolling_std3', 'memory_utilizationrolling_mean5', 'memory_utilizationrolling_std5', 'memory_pressurerolling_mean5', 'memory_pressurerolling_std5', 'memory_utilizationrolling_mean10', 'memory_utilizationrolling_std10', 'memory_pressurerolling_mean10', 'memory_pressurerolling_std10', 'resource_exhaustion', 'memory_pressure_failure', 'other_failure']\n",
            "Running anomaly detection...\n",
            "Detected 20018 anomalies out of 405894 records (4.93%)\n",
            "Correlation between anomalies and failures: -0.0256\n",
            "Before normalization, shape of features: (405894, 32)\n",
            "After normalization, shape of features: (405894, 32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0           time  instance_events_type  collection_id  \\\n",
              "0      183421              0                     2   152917495628   \n",
              "1      256265   352321459944                     3   261561475113   \n",
              "2       44099   365626782748                     3   261561475113   \n",
              "3      145811  1846171586901                     3   261561475113   \n",
              "4      154472  2571380789305                     3   261561475113   \n",
              "\n",
              "   scheduling_class  collection_type  priority  alloc_collection_id  \\\n",
              "0                 3                1       200                    0   \n",
              "1                 2                1       101                    0   \n",
              "2                 2                1       101                    0   \n",
              "3                 2                1       101                    0   \n",
              "4                 2                1       101                    0   \n",
              "\n",
              "   instance_index    machine_id  ... memory_pressurerolling_std5  \\\n",
              "0              47   17377689713  ...                         0.0   \n",
              "1             425  178160671591  ...                         0.0   \n",
              "2             257     813863542  ...                         0.0   \n",
              "3             263     527532269  ...                         0.0   \n",
              "4             135     559903004  ...                         0.0   \n",
              "\n",
              "  memory_utilizationrolling_mean10  memory_utilizationrolling_std10  \\\n",
              "0                              0.0                              0.0   \n",
              "1                              0.0                              0.0   \n",
              "2                              0.0                              0.0   \n",
              "3                              0.0                              0.0   \n",
              "4                              0.0                              0.0   \n",
              "\n",
              "  memory_pressurerolling_mean10 memory_pressurerolling_std10  \\\n",
              "0                           0.0                          0.0   \n",
              "1                           0.0                          0.0   \n",
              "2                           0.0                          0.0   \n",
              "3                           0.0                          0.0   \n",
              "4                           0.0                          0.0   \n",
              "\n",
              "  resource_exhaustion memory_pressure_failure  other_failure  anomaly_score  \\\n",
              "0                   0                       0              1              1   \n",
              "1                   0                       0              0              1   \n",
              "2                   0                       0              0              1   \n",
              "3                   0                       0              0              1   \n",
              "4                   0                       0              0              1   \n",
              "\n",
              "  is_anomaly  \n",
              "0          0  \n",
              "1          0  \n",
              "2          0  \n",
              "3          0  \n",
              "4          0  \n",
              "\n",
              "[5 rows x 67 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-deab723d-8a69-420e-af33-58919ebfcb6a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>time</th>\n",
              "      <th>instance_events_type</th>\n",
              "      <th>collection_id</th>\n",
              "      <th>scheduling_class</th>\n",
              "      <th>collection_type</th>\n",
              "      <th>priority</th>\n",
              "      <th>alloc_collection_id</th>\n",
              "      <th>instance_index</th>\n",
              "      <th>machine_id</th>\n",
              "      <th>...</th>\n",
              "      <th>memory_pressurerolling_std5</th>\n",
              "      <th>memory_utilizationrolling_mean10</th>\n",
              "      <th>memory_utilizationrolling_std10</th>\n",
              "      <th>memory_pressurerolling_mean10</th>\n",
              "      <th>memory_pressurerolling_std10</th>\n",
              "      <th>resource_exhaustion</th>\n",
              "      <th>memory_pressure_failure</th>\n",
              "      <th>other_failure</th>\n",
              "      <th>anomaly_score</th>\n",
              "      <th>is_anomaly</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>183421</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>152917495628</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>200</td>\n",
              "      <td>0</td>\n",
              "      <td>47</td>\n",
              "      <td>17377689713</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>256265</td>\n",
              "      <td>352321459944</td>\n",
              "      <td>3</td>\n",
              "      <td>261561475113</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>101</td>\n",
              "      <td>0</td>\n",
              "      <td>425</td>\n",
              "      <td>178160671591</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>44099</td>\n",
              "      <td>365626782748</td>\n",
              "      <td>3</td>\n",
              "      <td>261561475113</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>101</td>\n",
              "      <td>0</td>\n",
              "      <td>257</td>\n",
              "      <td>813863542</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>145811</td>\n",
              "      <td>1846171586901</td>\n",
              "      <td>3</td>\n",
              "      <td>261561475113</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>101</td>\n",
              "      <td>0</td>\n",
              "      <td>263</td>\n",
              "      <td>527532269</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>154472</td>\n",
              "      <td>2571380789305</td>\n",
              "      <td>3</td>\n",
              "      <td>261561475113</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>101</td>\n",
              "      <td>0</td>\n",
              "      <td>135</td>\n",
              "      <td>559903004</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 67 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-deab723d-8a69-420e-af33-58919ebfcb6a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-deab723d-8a69-420e-af33-58919ebfcb6a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-deab723d-8a69-420e-af33-58919ebfcb6a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-60e3b352-63f2-4411-86e6-e6cc6e07e48a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-60e3b352-63f2-4411-86e6-e6cc6e07e48a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-60e3b352-63f2-4411-86e6-e6cc6e07e48a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_scaled"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for target in target_cols:\n",
        "    print(f\"{target} distribution:\\n\", df_scaled[target].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY99_WNRjA3S",
        "outputId": "8909b466-d567-4eda-b751-1ccfb911976c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "failed distribution:\n",
            " failed\n",
            "0    313216\n",
            "1     92678\n",
            "Name: count, dtype: int64\n",
            "resource_exhaustion distribution:\n",
            " resource_exhaustion\n",
            "0    405894\n",
            "Name: count, dtype: int64\n",
            "memory_pressure_failure distribution:\n",
            " memory_pressure_failure\n",
            "0    405894\n",
            "Name: count, dtype: int64\n",
            "other_failure distribution:\n",
            " other_failure\n",
            "0    313216\n",
            "1     92678\n",
            "Name: count, dtype: int64\n",
            "is_anomaly distribution:\n",
            " is_anomaly\n",
            "0    385876\n",
            "1     20018\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 10\n",
        "target_cols = ['failed', 'resource_exhaustion', 'memory_pressure_failure', 'other_failure', 'is_anomaly']\n",
        "X, y_dict = create_sequences(df_scaled, features, sequence_length, target_cols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMdwEKVVazQI",
        "outputId": "df760110-4463-4cad-a518-8958121db703"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created sequences: X shape = (405884, 10, 32)\n",
            "y_failed shape = (405884,)\n",
            "Positive samples for failed: 92677 (22.83%)\n",
            "y_resource_exhaustion shape = (405884,)\n",
            "Positive samples for resource_exhaustion: 0 (0.00%)\n",
            "y_memory_pressure_failure shape = (405884,)\n",
            "Positive samples for memory_pressure_failure: 0 (0.00%)\n",
            "y_other_failure shape = (405884,)\n",
            "Positive samples for other_failure: 92677 (22.83%)\n",
            "y_is_anomaly shape = (405884,)\n",
            "Positive samples for is_anomaly: 20018 (4.93%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Training**"
      ],
      "metadata": {
        "id": "bazoLH9Y7WaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train_and_select_best_model(df_scaled, features, target_cols, model_constructors):\n",
        "    \"\"\"\n",
        "    Trains multiple models on each target variable, selects the best-performing model based on weighted F1 score,\n",
        "    and saves that model to disk.\n",
        "\n",
        "    Parameters:\n",
        "        df_scaled (DataFrame): Preprocessed and normalized dataset.\n",
        "        features (list): List of feature column names.\n",
        "        target_cols (list): List of target column names (e.g., ['failed', 'is_anomaly']).\n",
        "        model_constructors (dict): Dictionary of model names and their corresponding constructors.\n",
        "\n",
        "    Returns:\n",
        "        best_models (dict): A dictionary where for each target, the best model is stored along with its name and F1 score.\n",
        "    \"\"\"\n",
        "    best_models = {}\n",
        "\n",
        "    for target in target_cols:\n",
        "        best_f1 = -1\n",
        "        best_model = None\n",
        "        best_model_name = \"\"\n",
        "        print(f\"\\n=== Evaluating models for target: {target} ===\")\n",
        "\n",
        "        # Extract features and target\n",
        "        X = df_scaled[features].values\n",
        "        y = df_scaled[target].values\n",
        "\n",
        "        # Split data into training and testing sets (80/20 split)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        for model_name, model_constructor in model_constructors.items():\n",
        "            print(f\"\\nTraining model: {model_name}\")\n",
        "            model = model_constructor\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "            print(f\"{model_name} weighted F1 score: {f1:.4f}\")\n",
        "\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_model = model\n",
        "                best_model_name = model_name\n",
        "\n",
        "        best_models[target] = {\"model\": best_model, \"model_name\": best_model_name, \"f1_score\": best_f1}\n",
        "        print(f\"\\nBest model for {target}: {best_model_name} with weighted F1 score: {best_f1:.4f}\")\n",
        "        # Save the best model to disk using pickle\n",
        "        with open(f\"best_model_{target}.pkl\", \"wb\") as f:\n",
        "            pickle.dump(best_model, f)\n",
        "\n",
        "    return best_models\n",
        "\n",
        "# Define model constructors for various models (without SVM)\n",
        "model_constructors = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "}\n",
        "\n",
        "# Specify the targets you want to evaluate:\n",
        "anomaly_targets = ['failed', 'is_anomaly']\n",
        "\n",
        "# Now, assuming you've already prepared and scaled your data:\n",
        "# df = load_data(\"borg_traces_data.csv\")\n",
        "# df = preprocess_time(df, time_col='start_time')\n",
        "# df, k8s_mapping = map_to_kubernetes(df)\n",
        "# df, features = engineer_features(df)\n",
        "# df = detect_anomalies(df, features)\n",
        "# df_scaled, scaler = normalize_features(df, features)\n",
        "\n",
        "best_models = train_and_select_best_model(df_scaled, features, anomaly_targets, model_constructors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY3Fj7kcjA5o",
        "outputId": "1ce3c738-00ea-49c8-f557-212da517d9fa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluating models for target: failed ===\n",
            "\n",
            "Training model: Logistic Regression\n",
            "Logistic Regression weighted F1 score: 0.6699\n",
            "\n",
            "Training model: Random Forest\n",
            "Random Forest weighted F1 score: 0.9979\n",
            "\n",
            "Training model: Decision Tree\n",
            "Decision Tree weighted F1 score: 0.9976\n",
            "\n",
            "Training model: XGBoost\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:32:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost weighted F1 score: 0.9905\n",
            "\n",
            "Best model for failed: Random Forest with weighted F1 score: 0.9979\n",
            "\n",
            "=== Evaluating models for target: is_anomaly ===\n",
            "\n",
            "Training model: Logistic Regression\n",
            "Logistic Regression weighted F1 score: 0.9926\n",
            "\n",
            "Training model: Random Forest\n",
            "Random Forest weighted F1 score: 0.9995\n",
            "\n",
            "Training model: Decision Tree\n",
            "Decision Tree weighted F1 score: 0.9993\n",
            "\n",
            "Training model: XGBoost\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:33:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost weighted F1 score: 0.9995\n",
            "\n",
            "Best model for is_anomaly: XGBoost with weighted F1 score: 0.9995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM-Model**"
      ],
      "metadata": {
        "id": "Uap3KBZHeJze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "def build_lstm_model(seq_length, num_features):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=(seq_length, num_features), return_sequences=True))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(32))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # For binary classification\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = build_lstm_model(seq_length=10, num_features=len(features))\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "Kj9l3_WPcZtP",
        "outputId": "8c764668-29a0-40c8-f0bb-94061985a060"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │          \u001b[38;5;34m24,832\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │          \u001b[38;5;34m12,416\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │             \u001b[38;5;34m528\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m17\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">24,832</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m37,793\u001b[0m (147.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,793</span> (147.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m37,793\u001b[0m (147.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,793</span> (147.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V9XHvtRcc1E",
        "outputId": "8ebd885f-7f11-4b2f-8ccf-7bb4df056d19"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 14ms/step - accuracy: 0.7929 - loss: 0.5009 - val_accuracy: 0.7914 - val_loss: 0.5143\n",
            "Epoch 2/10\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 13ms/step - accuracy: 0.8030 - loss: 0.4584 - val_accuracy: 0.7394 - val_loss: 0.6065\n",
            "Epoch 3/10\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 17ms/step - accuracy: 0.8228 - loss: 0.4073 - val_accuracy: 0.6166 - val_loss: 0.6338\n",
            "Epoch 4/10\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 14ms/step - accuracy: 0.8348 - loss: 0.3757 - val_accuracy: 0.6437 - val_loss: 0.7491\n",
            "Epoch 5/10\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 13ms/step - accuracy: 0.8483 - loss: 0.3438 - val_accuracy: 0.6358 - val_loss: 0.7754\n",
            "Epoch 6/10\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 13ms/step - accuracy: 0.8572 - loss: 0.3238 - val_accuracy: 0.6384 - val_loss: 0.7110\n",
            "Epoch 7/10\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 15ms/step - accuracy: 0.8680 - loss: 0.3017 - val_accuracy: 0.6176 - val_loss: 0.8375\n",
            "Epoch 8/10\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 14ms/step - accuracy: 0.8761 - loss: 0.2839 - val_accuracy: 0.3887 - val_loss: 0.8971\n",
            "Epoch 9/10\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 13ms/step - accuracy: 0.8865 - loss: 0.2622 - val_accuracy: 0.5124 - val_loss: 0.8454\n",
            "Epoch 10/10\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 13ms/step - accuracy: 0.8956 - loss: 0.2465 - val_accuracy: 0.3841 - val_loss: 0.9386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH-i56PIchf7",
        "outputId": "a2d7d5fd-6c2b-44b4-e3f6-18032ee915f9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.9150, Test Accuracy: 0.6252\n"
          ]
        }
      ]
    }
  ]
}